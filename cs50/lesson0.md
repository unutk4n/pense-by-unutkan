computational thinking

byte = 00000000
bit = 0
How to represent "A" in bytes? >> 01000001 >> 65
ASCII is the way to represent the letters of the alphabet.
unicode
rgb
algorithm >> code